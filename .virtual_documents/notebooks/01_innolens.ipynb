





OPENAI_MODEL = "gpt-4o-mini"


import os
from dotenv import load_dotenv
from openai import OpenAI as OpenAIClient  # Rename for clarity
from openai.types.responses import Response
import json

def load_openai_client() -> OpenAIClient:
    load_dotenv()
    api_key: str = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY not found in environment variables.")
    return OpenAIClient(api_key=api_key)

def extract_search_terms(research_objective: str,
                         client: OpenAIClient = load_openai_client(),
                         model: str = OPENAI_MODEL
                         )-> dict:

    # Prompt instruction to get the key words
    prompt: str = f"""
    You are assisting a researcher in gener,ating targeted search terms for academic and patent literature related to the research topic described below.

    Return a JSON object with the following structure:
    - "main_topic": a concise list with one element of words (2–3 words) in singular, extracted from the Research Objective that reflects the core technological focus
    - "secondary_topic": exactly 5 single word search terms, extracted from the Research Objective that reflects the client objective
    
    Instructions:
    - Output must be valid JSON only — no markdown, comments, or extra text
    - All terms in "secondary_topic" must be single word
    - Do NOT include any words or close variants from "main_topic" in "secondary_topic"
    - All terms across the fields must be unique — no repetition or synonyms
    - Each term in "secondary_topic" must be conceptually compatible with the "main_topic" so that combining them (e.g. "main_topic" AND "keyword") produces a realistic and meaningful research query
    - Use language and terminology commonly found in scientific publications and patent documents

    Research Objective:
    \"\"\" 
    {research_objective} 
    \"\"\"
    """
        
    response: Response = client.responses.create(

        model=model, # Availables models: https://platform.openai.com/docs/pricing
        input = prompt
    )

    print(f"The lenght of research objective is: {len(research_objective)}")
   
    research_key_words: dict[str, list[str]]= json.loads(response.output_text)
    
    print(research_key_words)

    return research_key_words


# Research objective
CLIENT_OBJECTIVE: str = """
At present, one of our clients is looking to speak with professionals who have insights about the emerging technologies 
in soft contact lens manufacturing, particularly non-injection moulded methods. They would broadly like to understand how 
these technologies are reshaping the industry—from on-demand manufacturing to smart, drug-delivery-enabled lenses.
"""


search_terms_dict = extract_search_terms(CLIENT_OBJECTIVE)





import json

# Print the main topic
search_terms_dict["main_topic"] = ["soft contact lens"] # I change the main topic because the llm did not found the correct topic
print(search_terms_dict["main_topic"])
# print(search_terms_dict["secondary_topic"])

new_terms = ['manufacturing', 'mold', 'injection', 'drug'] # I add these key words because I think the more relevant
# search_terms_dict["secondary_topic"].extend(new_terms)
search_terms_dict["secondary_topic"] = new_terms
print(search_terms_dict["secondary_topic"])











# Constants
OPENALEX_URL:str = "https://api.openalex.org/works"
MAILTO: str = "adyl.elguamra@gmail.com"
OPENALEX_PER_PAGE: int = 50
PAGE: int = 1





import requests
import pandas as pd
import time
import json
from typing import Any


def openalex_query_api(
    query_result: str,
    url: str,
    keyword: str,
    headers: dict[str, str] | None,
    params: dict[str, str],
    verbose: bool = False
) -> list[dict]:
    """
    Sends a GET request to the OpenAlex API and returns the list of results.
    """
    try:
        response: requests.Response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data: dict[str, Any] = response.json()

        if verbose:
            print("Type of data:", type(data))  # <class 'dict'>
            print("Type of 'results' field:", type(data.get("results")))  # list or None
            print("Pretty JSON:", json.dumps(data, indent=4))

        return data.get(query_result, [])

    except requests.exceptions.RequestException as e:
        print(f"Error for keyword '{keyword}': {e}")
        return []

def fetch_openalex_data(
    main_topic: str,
    secondary_keywords: list[str],
    query_result: str = "results",
    url: str = OPENALEX_URL,
    mailto: str = MAILTO,
    per_page: int = OPENALEX_PER_PAGE,
    page: int = PAGE,
    delay: float = 1.0,
    verbose: bool = True
) -> pd.DataFrame | None:
    """
    Queries OpenAlex for each secondary keyword combined with the main topic,
    and returns a combined DataFrame of results.
    """
    dfs: list[pd.DataFrame] = []

    for kw in secondary_keywords:
        print(f"\nSearching for: '{main_topic} {kw}'")

        params: dict[str, str | int] = {
            "search": f"{main_topic} {kw}",
            "per_page": per_page,  # int
            "page": page,          # int
            "sort": "relevance_score:desc",
            "mailto": mailto
        }

        papers: list[dict] = openalex_query_api(
            query_result=query_result,
            url=url,
            keyword=kw,
            headers=None,
            params=params,
            verbose=verbose
        )

        if papers:
            df = pd.DataFrame(papers)
            df["keyword"] = kw
            dfs.append(df)

        time.sleep(delay)

    if dfs:
        result_df = pd.concat(dfs, ignore_index=True)
        print(f"Final DataFrame shape: {result_df.shape}")
        return result_df

    print("No results found.")
    return None

# Launch the paper search
openalex_raw_df = fetch_openalex_data(
    main_topic=search_terms_dict["main_topic"][0],
    secondary_keywords=search_terms_dict["secondary_topic"]
)





def openalex_filter_col(df: pd.DataFrame)->pd.DataFrame:

    print(df.columns.tolist())
    openalex_columns_to_keep: list[str] = ["keyword","title", "publication_date", "authorships", "abstract_inverted_index"]
    filtered_df: pd.DataFrame = df[openalex_columns_to_keep].copy()
    
    return filtered_df   

openalex_subset_df: pd.DataFrame = openalex_filter_col(openalex_raw_df)
openalex_subset_df.head()





def safe_get(nested_obj: list | dict, path: list[int | str], default=None) -> str:
    """
    Safely get a value from nested lists/dicts.
    """

    try:
        for key in path:
            if isinstance(nested_obj, list) and isinstance(key, int):
                nested_obj = nested_obj[key]
            elif isinstance(nested_obj, dict):
                nested_obj = nested_obj[key]
            else:
                return default
        return nested_obj
    except (TypeError, IndexError, KeyError):
        return default


print("Started the extraction: author_display_name...")
openalex_subset_df['first_author_display_name'] = openalex_subset_df['authorships'].apply(
    lambda x: safe_get(x, [0, 'author', 'display_name'])
)

print("Started the extraction: last_author_display_name...")
openalex_subset_df['last_author_display_name'] = openalex_subset_df['authorships'].apply(
    lambda x: safe_get(x, [1, 'author', 'display_name'])
)

print("Started the extraction: institution_display_name...")
openalex_subset_df['institution_display_name'] = openalex_subset_df['authorships'].apply(
    lambda x: safe_get(x, [0, 'institutions', 0, 'display_name'])
)

print("Started the extraction: institution_country_code...")
openalex_subset_df['institution_country_code'] = openalex_subset_df['authorships'].apply(
    lambda x: safe_get(x, [0, 'institutions', 0, 'country_code'])
)





def reconstruct_abstract(abstract_inverted_index: dict[str, list[int]]) -> str:
    '''
    Reconstruct the abstract from abstract_inverted_index
    '''
    
    # Some works don't have an abstract
    if not abstract_inverted_index:
        return ""
        
    # Variable to store the highest index
    max_value: int = 0
     # Loop through all the list of position in the abstract_inverted_index dictionary.
    for values in abstract_inverted_index.values():
        # Loop through all the index value
        for value in values:
            # identify the highest value index
            if value >= max_value:
                max_value = value
                
    # Create an empty list with abstract size        
    abstract: list[str] = [None] * (max_value +1)

    # Loop through each word in the abstract_inverted_index:
    for word, positions in abstract_inverted_index.items():
        # For each word, get the list of positions it appears in.
        for position in positions:
            # Insert each word into its correct position in the list.
            abstract[position]= word
                        
    # Join all the words in the list into a single string, separated by spaces.
    # abstract_text: str = " ".join(abstract)
    abstract_text: str = " ".join(word if word is not None else "" for word in abstract)

    # print("\n", abstract_text)
    
    return abstract_text

# Extract abstract
# Apply your reconstruct_abstract function
print("Abtract extration started...")
openalex_subset_df.loc[:, 'abstract'] = openalex_subset_df['abstract_inverted_index'].apply(
    reconstruct_abstract
)
print("Abtract extration complete.")


# Create a new datafram
openalex_notclean_df = openalex_subset_df[['keyword', 
                                           'title',
                                           'abstract',
                                           'publication_date', 
                                           'first_author_display_name', 
                                           'last_author_display_name', 
                                           'institution_display_name', 
                                           'institution_country_code' 
                                           ]].copy()

print(openalex_notclean_df.shape)
openalex_notclean_df.head()





print(f"The dataframe shape is {openalex_notclean_df.shape}")


import pandas as pd

def check_and_remove_duplicates(df: pd.DataFrame, col: str) -> pd.DataFrame:

    print("Dirty dataframe shape:", df.shape)
    duplicates: pd.DataFrame = df[df.duplicated(subset=col, keep=False)]

    if not duplicates.empty:
        print(f"\nFound {duplicates.shape[0]} duplicate rows based on column '{col}':")
        # print(duplicates)
        df_cleaned: pd.DataFrame = df.drop_duplicates(subset=col, keep="first").reset_index(drop=True)
        print(f"\n Removed duplicates. New dataframe shape: {df_cleaned.shape}")
        return df_cleaned
    else:
        print(f"\nNo duplicates found in '{col}'.")
        print(f"The dataframe shape is {df.shape}")
        return df
      
no_relevant_filter_openalex_df = check_and_remove_duplicates(openalex_notclean_df, col="title")


# Count None or NaN in 'applicant_organization' column
none_count_openalex: int = no_relevant_filter_openalex_df['institution_display_name'].isna().sum()

print(f"Number of None/NaN values in 'applicant_organization' column: {none_count_openalex}")


no_relevant_filter_openalex_df.head()





# Create a new column: will inform if the paper is relevant or not
no_relevant_filter_openalex_df["relevant"] = ""
# Create a new column: will provide the relevance justification
no_relevant_filter_openalex_df["justification"] = ""
# print the shape
print("Dataframe shape:",no_relevant_filter_openalex_df.shape)
# Print first rows of dataframe
no_relevant_filter_openalex_df[["title", "publication_date", "institution_country_code", "institution_display_name"]].head()



import pandas as pd

def get_justification(title: str, abstract: str, objective:str)-> str:
    prompt = f"""
    You are the CEO, as well as a scientific and regulatory analyst, evaluating academic research for a company .

    Below is a paper's title and abstract, followed by the company's research objective. 
    
    Title:  
    {title}

    Abstract:  
    {abstract}

    Research Objective:  
    {objective}

    Assess if the paper is relevant. Start the answer with Yes or No, followed by a 1–2 sentence explanation based only on the title and abstract. 
    Be concise, specific, and fact-based. Avoid speculation or vague generalizations.
    """

    # print("Generating GPT justifications...")
    
    client: OpenAIClient = load_openai_client()
    
    try:
        response: Response = client.responses.create(
        model="gpt-4o-mini",
        input = prompt
        )
        return response.output_text
    except Exception as e:
        return f"ERROR: {e}"
    
def process_row(title: str, abstract: str, objective: str) -> pd.Series:
    output = get_justification(title, abstract, objective)
    output_lower = output.lower()

    if output_lower.startswith("yes"):
        relevant = "yes"
    elif output_lower.startswith("no"):
        relevant = "no"
    else:
        relevant = "unclear"

    return pd.Series({"relevant": relevant, "justification": output})

# Apply the function to the entire DataFrame

relevants = []
justifications = []

for _, row in no_relevant_filter_openalex_df.iterrows():
    result = process_row(row["title"], row["abstract"], CLIENT_OBJECTIVE)
    relevants.append(result["relevant"])
    justifications.append(result["justification"])

no_relevant_filter_openalex_df["relevant"] = relevants
no_relevant_filter_openalex_df["justification"] = justifications



# Show full column contents and more columns
#pd.set_option("display.max_colwidth", None)    # Show full text in cells
print("The dataframe shape of all papers:", no_relevant_filter_openalex_df.shape)

# Filter relevant papers
openalex_df: pd.DataFrame = no_relevant_filter_openalex_df[no_relevant_filter_openalex_df['relevant'].str.lower() == 'yes'].copy()

# Print shape
print(f"The dataframe shape of relevant papers: {openalex_df.shape}")

# Display first rows
openalex_df[["keyword", "title", "institution_country_code", "institution_display_name", "publication_date","relevant", "justification"]].head(10)



def missing_report(df: pd.DataFrame, column_name: str)-> None:
    """
    Print total rows, missing count, and percentage missing for a given column.
    """
    total_rows: int = len(df)
    missing_count: int = df[column_name].isna().sum()
    percent_missing: int = (missing_count / total_rows) * 100

    print(f"Column: {column_name}")
    print(f"Total rows: {total_rows}")
    print(f"Missing: {missing_count}")
    print(f"Percentage missing: {percent_missing:.2f}%")
    
missing_report(openalex_df, "institution_display_name")


# convert 'publication_date' column to datetime format
openalex_df['publication_date'] = pd.to_datetime(openalex_df['publication_date'])

# Extract the year and store it in a new column 'year'
openalex_df['year'] = openalex_df['publication_date'].dt.year

# Let's check the result
openalex_df.head()



# Save cleaned dataframe to CSV inside data folder
openalex_df.to_csv("../data/openalex_df.csv", index=False)

print("Cleaned dataframe saved to data/patentview_df_cleaned.csv")











import requests
import pandas as pd
from dotenv import load_dotenv
import os

# Load API key
load_dotenv()

PATENTVIEW_API_KEY: str = os.getenv("PATENTSVIEW_API_KEY")

# Define the number of results retuiurned per request
PATENTVIEW_SIZE: int = 1000

# Endpoint and headers
PATENTVIEW_URL: str = "https://search.patentsview.org/api/v1/patent" # url = "https://search.patentsview.org/api/v1/patent"
PATENTVIEW_HEADERS: dict[str, str] = {
    "X-Api-Key": PATENTVIEW_API_KEY,
    "Content-Type": "application/json"
}





import requests
import json
import pandas as pd
import os
from dotenv import load_dotenv

# Load API key
load_dotenv()

def fetch_patentview_data(search_term: str,
                          url: str = PATENTVIEW_URL,
                          headers: dict[str, str] = PATENTVIEW_HEADERS,
                          size: int = PATENTVIEW_SIZE, # specifies the number of results per page
                          verbose: bool = False
                          ) -> pd.DataFrame:
    """
    Search the PatentsView API for patents mentioning a term in the abstract.
    Returns a DataFrame with up to 1,000 records, including the keyword used.
    """

    query: dict[str, dict[str,str]] = {
        "_text_phrase": {
            "patent_abstract": search_term
        }
    }

    fields: list[str] = [
        "patent_id", "patent_title", "patent_date", "patent_abstract",
        "applicants", "application", "assignees", "attorneys", "cpc_current", 
    ]

    params: dict[str, str] = {
        "q": json.dumps(query),
        "f": json.dumps(fields),
        "o": json.dumps({"size": size}),
        "s": json.dumps([{"patent_date": "desc"}])
    }

    # Send a get request
    response: requests.Response = requests.get(url, headers=headers, params=params)
    response.raise_for_status()
    
    if response.status_code == 200:
        
        data: dict[str, Any] = response.json()
        results: list[dict[str, Any]] = data.get("patents", [])
        
        if verbose:
            print("Type of data:", type(data))  # <class 'dict'>
            print("Type of 'results' field:", type(data.get("patents")))  # list or None
            print("Pretty JSON:", json.dumps(data, indent=4))
        
        df = pd.DataFrame(results)
        df["main_keyword"] = search_term
        print(f"Found {len(df)} patents for '{search_term}'")
        return df
    else:
        print(f"Request failed with status {response.status_code}")
        return pd.DataFrame()


# Fetche the patview data
patentview_subset_df = fetch_patentview_data(search_term = search_terms_dict["main_topic"][0], 
                                             verbose=True)


print(patentview_subset_df.head())


print(patentview_subset_df.shape)
patentview_subset_df[["patent_id", "patent_title", "main_keyword"]].head()
# research_key_words["patentview"]



patentview_subset_df.tail()


print(patentview_subset_df.columns.tolist())


patentview_subset_df[["patent_id", "patent_title", "patent_date", 'applicants']].head()





# Extract applicant_organization
patentview_subset_df.loc[:, 'assignee_organization'] = patentview_subset_df['assignees'].apply(
    lambda x: safe_get(x, [0, 'assignee_organization'])
)

# Extract first assignee_country
patentview_subset_df.loc[:, 'assignee_country'] = patentview_subset_df['assignees'].apply(
    lambda x: safe_get(x, [0, 'assignee_country'])
)

# Extract cpc_group_id
patentview_subset_df.loc[:, 'cpc_group_id'] = patentview_subset_df['cpc_current'].apply(
    lambda x: safe_get(x, [0, 'cpc_group_id'])
)

patentview_subset_df[["patent_id", "patent_title", "patent_date", "assignee_organization", "assignee_country","cpc_group_id"]].head()


patentview_subset_df.head()


print(patentview_subset_df.columns.tolist())


# Create a new datafram
patentview_notclean_df = patentview_subset_df[['main_keyword',
                                               'patent_id', 
                                               'patent_title',
                                               'patent_date',
                                               'patent_abstract',
                                               #'applicants',
                                               'assignee_organization',
                                               'assignee_country',
                                               'cpc_group_id'
                                                ]].copy()

print(patentview_notclean_df.shape)
patentview_notclean_df.head()


missing_report(patentview_notclean_df, "assignee_organization")





# Step 1: remove duplicated rows
patentview_df: pd.DataFrame =  check_and_remove_duplicates(patentview_notclean_df, col="patent_title")

missing_report(patentview_df, "assignee_organization")


# Count None or NaN in 'applicant_organization' column
none_count: int = patentview_df['assignee_organization'].isna().sum()

print(f"Number of None/NaN values in 'applicant_organization' column: {none_count}")

# # Remove rows where applicant_organization is NaN / None
# patentview_df_cleaned = patentview_df.dropna(subset=['applicant_organization'])

# # Check new shape
# print(f"New dataframe shape after removing missing applicant_organization: {patentview_df_cleaned.shape}")



# convert 'publication_date' column to datetime format
patentview_df['patent_date'] = pd.to_datetime(patentview_df['patent_date'])

# Extract the year and store it in a new column 'year'
patentview_df['year'] = patentview_df['patent_date'].dt.year

# Let's check the result
patentview_df.head()






import pandas as pd

# Load the CPC title table
cpc_titles: pd.DataFrame = pd.read_csv('../data/g_cpc_title.tsv', sep='\t')
cpc_titles.head()


# Rename column in cpc_titles to match your patentview_df
cpc_titles.rename(columns={'cpc_group': 'cpc_group_id'}, inplace=True)


# create a mapping dic
cpc_dict: dict[str, str] = dict(
    zip(
        cpc_titles['cpc_group_id'],
        cpc_titles['cpc_group_title']
    )
)

patentview_df.loc[:,'cpc_group_title'] = patentview_df['cpc_group_id'].map(cpc_dict)
patentview_df.head()


missing_report(patentview_df, "cpc_group_title")


print(patentview_df.dtypes)



# convert 'publication_date' column to datetime format
patentview_df['patent_date'] = pd.to_datetime(patentview_df['patent_date'])
print(patentview_df.dtypes)

# # Extract the year and store it in a new column 'year'
# openalex_df['year'] = openalex_df['publication_date'].dt.year

# Let's check the result
patentview_df.head()





# Save cleaned dataframe to CSV inside data folder
patentview_df.to_csv("../data/patentview_df.csv", index=False)

print("Cleaned dataframe saved to data/patentview_df_cleaned.csv")



patentview_df.head()








FDA_BASE_URL: str = "https://api.fda.gov/device/510k.json"
FDA_LIMIT: int = 1000 #number of records that match the search parameter. 1000 is the largest allowed





import requests
import pandas as pd

def search_fda_510k(keyword: str, 
                    base_url: str = FDA_BASE_URL,
                    limit =FDA_LIMIT,
                    verbose: bool = False) -> pd.DataFrame:
    """
    Searches the FDA 510(k) database for devices matching the keyword.
    Returns a DataFrame with up to 1,000 results.
    """   
    # Query: wildcard match on keyword in device name and date range
    query: str = f'device_name:{keyword} AND decision_date:[1990-01-01 TO 2025-01-01]'
    
    params = {
        'search': query,
        'limit': limit
    }

    response: requests.Response = requests.get(base_url, params=params)
    
    response.raise_for_status()
    
    if response.status_code == 200:
        
        data: dict[str, Any] = response.json()
        results: list[dict[str, Any]] = data.get("results", [])
        
        if verbose:
            print("Type of data:", type(data))  # <class 'dict'>
            print("Type of 'results' field:", type(data.get("results")))  # list or None
            print("Pretty JSON:", json.dumps(data, indent=4))
        
        df = pd.DataFrame(results)
        df["main_keyword"] = keyword
        print(f"Found {len(df)} patents for '{keyword}'")
        return df
    else:
        print(f"Request failed with status {response.status_code}")
        return pd.DataFrame() 

# Run the FDA search
fda_notclean_df: pd.DataFrame = search_fda_510k(search_terms_dict["main_topic"][0], verbose = True)


# print the list of columns
print(fda_notclean_df.columns.to_list())

# Preview
fda_notclean_df[['device_name', 'applicant', 'decision_date', 'k_number', 'decision_description', 'contact', 'product_code','country_code']].head()





# Step 1: remove duplicated rows
fda_no_duplicates_df: pd.DataFrame =  check_and_remove_duplicates(fda_notclean_df, col="k_number")
# Keep selected columsn
fda_no_duplicates_df: pd.DataFrame = fda_no_duplicates_df[['device_name', 'applicant', 'decision_date', 'k_number', 'decision_description', 'contact', 'product_code','country_code']].copy()
# Preview
fda_no_duplicates_df.head()


# convert 'publication_date' column to datetime format
fda_no_duplicates_df ['decision_date'] = pd.to_datetime(fda_no_duplicates_df ['decision_date'])

# Extract the year and store it in a new column 'year'
fda_no_duplicates_df ['year'] = fda_no_duplicates_df ['decision_date'].dt.year

# Let's check the result
fda_no_duplicates_df.head()


print(fda_no_duplicates_df .dtypes)



# Now safely convert to datetime
fda_no_duplicates_df ['decision_date'] = pd.to_datetime(fda_no_duplicates_df ['decision_date'])

print(fda_no_duplicates_df .dtypes)





import pandas as pd

fda_product_code_df = pd.read_excel("../data/no_careproducts_PCDExcelReport26.xls", sheet_name="Sheet1")
print("The number of product code:", len(fda_product_code_df))
fda_product_code_df.head(23)


import pandas as pd


# Extract the list of valid product codes
valid_codes: list[str] = fda_product_code_df["Product Code"].astype(str).tolist()

# Filter your main FDA DataFrame
fda_df: pd.DataFrame = fda_no_duplicates_df[fda_no_duplicates_df["product_code"].isin(valid_codes)].copy()


# Inspect the result
print(f"Rows before filtering: {len(fda_df)}")
print(f"Rows after filtering:  {len(fda_df)}")
print(fda_df.head())





desc_map = dict(
    zip(
        fda_product_code_df["Product Code"],
        fda_product_code_df["Device"]
    )
)

fda_df.loc[:,"product_description"] = fda_df["product_code"].map(desc_map)

fda_df.head()


fda_df["product_code"].unique()





# Save cleaned dataframe to CSV inside data folder
fda_df.to_csv("../data/fda_df.csv", index=False)

print("Cleaned dataframe saved to data/fda_df.csv")


fda_df.head()








openalex_df.head()





import pandas as pd
import matplotlib.pyplot as plt

def plot_papers_per_year_by_keyword(
    df: pd.DataFrame,
    show_inline: bool = False
) -> plt.Figure:
    """
    Creates a stacked bar chart showing the number of papers published per year,
    grouped by keyword. Returns the matplotlib Figure object.
    """
    # Make a copy to avoid mutating the original DataFrame
    data = df.copy()

    # Step 1: Group the data by year and keyword, count occurrences
    grouped_data = (
        data
        .groupby(['year', 'keyword'])
        .size()
        .unstack(fill_value=0)
    )

    # Step 2: Create a figure & axes
    fig, ax = plt.subplots(figsize=(12, 7))

    # Step 3: Plot the stacked bar chart
    grouped_data.plot(
        kind='bar',
        stacked=True,
        ax=ax
    )

    # Step 4: Add titles and axis labels
    ax.set_title('Number of Papers by Year and Keyword (Stacked)', fontsize=16)
    ax.set_xlabel('Year', fontsize=14)
    ax.set_ylabel('Number of Papers', fontsize=14)

    # Step 5: Format x-axis labels and legend
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
    ax.legend(title='Keyword', fontsize=10)

    # Step 6: Adjust layout
    plt.tight_layout()

    # Step 7: Show inline if requested
    if show_inline:
        plt.show()

    # Return the figure for Streamlit (or further use)
    return fig

# Example usage in Jupyter:
fig = plot_papers_per_year_by_keyword(openalex_df, show_inline=True)

# Example usage in Streamlit:
# fig = plot_papers_per_year_by_keyword(openalex_df, show_inline=False)
# st.pyplot(fig)






import pandas as pd
import matplotlib.pyplot as plt

def plot_top_institutions(
    df: pd.DataFrame,
    top_n: int = 10,
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots a horizontal bar chart of the top institutions by number of papers.
    """
    # Copy to avoid mutating original
    data = df.copy()

    # Step 1: Count papers per institution
    institution_counts = data['institution_display_name'].value_counts().head(top_n)

    # Step 2: Create figure and axes
    fig, ax = plt.subplots(figsize=(12, 7))

    # Step 3: Plot horizontal bar chart
    institution_counts.plot(kind='barh', ax=ax, color='skyblue')

    # Step 4: Add titles and labels
    ax.set_title(f'Top {top_n} Institutions by Number of Papers', fontsize=16)
    ax.set_xlabel('Number of Papers', fontsize=14)
    ax.set_ylabel('Institution', fontsize=14)

    # Step 5: Invert y-axis to have highest value on top
    ax.invert_yaxis()

    # Step 6: Adjust layout
    plt.tight_layout()

    # Step 7: Show inline in notebooks if requested
    if show_inline:
        plt.show()

    return fig

# Example usage in Jupyter:
fig = plot_top_institutions(openalex_df, top_n=10, show_inline=True)

# Example usage in Streamlit:
# fig = plot_top_institutions(openalex_df, top_n=10, show_inline=False)
# st.pyplot(fig)






import pandas as pd
import matplotlib.pyplot as plt

def plot_top_last_authors(
    df: pd.DataFrame,
    top_n: int = 10,
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots a horizontal bar chart of the top last authors by number of papers.

    Args:
        df (pd.DataFrame): DataFrame with 'last_author_display_name' column.
        top_n (int): Number of top authors to show (default is 10).
        show_inline (bool): If True, calls plt.show() so notebooks render the plot immediately.

    Returns:
        fig (plt.Figure): The Matplotlib figure object.
    """
    # Copy to avoid mutating original DataFrame
    data = df.copy()

    # Step 1: Count papers per last author
    last_author_counts = data['last_author_display_name'].value_counts().head(top_n)

    # Step 2: Create figure and axes
    fig, ax = plt.subplots(figsize=(12, 7))

    # Step 3: Plot horizontal bar chart
    last_author_counts.plot(kind='barh', ax=ax, color='skyblue')

    # Step 4: Add titles and labels
    ax.set_title(f'Top {top_n} Last Authors by Number of Papers', fontsize=16)
    ax.set_xlabel('Number of Papers', fontsize=14)
    ax.set_ylabel('Last Author', fontsize=14)

    # Step 5: Invert y-axis to have highest values at the top
    ax.invert_yaxis()

    # Step 6: Adjust layout
    plt.tight_layout()

    # Step 7: Show inline in notebooks if requested
    if show_inline:
        plt.show()

    return fig

# Example usage in Jupyter Notebook:
fig = plot_top_last_authors(openalex_df, top_n=10, show_inline=True)

# Example usage in Streamlit:
# fig = plot_top_last_authors(openalex_df, top_n=10, show_inline=False)
# st.pyplot(fig)






import pandas as pd
import matplotlib.pyplot as plt

def plot_top_countries_by_institution(
    df: pd.DataFrame,
    top_n: int = 10,
    show_inline: bool = False
) -> plt.Figure:
    """
    Creates a horizontal bar chart of the top countries by number of papers
    using the 'institution_country_code' column.
    """
    data = df.copy()

    if 'institution_country_code' not in data.columns:
        raise ValueError("Column 'institution_country_code' not found in the DataFrame.")

    # Count papers by country
    counts = data['institution_country_code'].value_counts().head(top_n)

    # Plot
    fig, ax = plt.subplots(figsize=(12, 7))
    counts.plot(kind='barh', ax=ax, color='lightgreen')

    ax.set_title(f"Top {top_n} Countries by Number of Papers", fontsize=16)
    ax.set_xlabel("Number of Papers", fontsize=14)
    ax.set_ylabel("Country Code", fontsize=14)
    ax.invert_yaxis()
    plt.tight_layout()

    if show_inline:
        plt.show()

    return fig

fig = plot_top_countries_by_institution(openalex_df, top_n=10, show_inline=True)









import pandas as pd
import json
from openai.types.responses import Response

def normalize_company_names(patentview_df: pd.DataFrame, fda_df: pd.DataFrame, verbose: bool = False) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Normalizes organization names in patentview and fda dataframes to their parent company names using OpenAI.
    Returns the modified dataframes with added columns: 'assignee_standardized' and 'applicant_standardized'.
    """
    # Step 1: Collect all unique company names from both DataFrames
    company_names = pd.concat([
        patentview_df['assignee_organization'],
        fda_df['applicant']
    ]).dropna().unique().tolist()

    name_list = '\n'.join(company_names)

    # Step 2: Prepare prompt
    prompt = f"""
    System:
    You are a company-name–normalization expert. Your job is to map raw, potentially messy organization names to their canonical parent company names.

    User:
    I have the following list of company names (one per line), which may include subsidiaries, legal suffixes (Inc, LLC, GmbH, etc.), or spelling variants:

    {name_list}

    Task:
    1. For each original name, determine the standardized parent company name.
    2. If the name is already the parent, map it to itself.
    3. Strip off any legal suffixes in the output (e.g. “Inc”, “LLC”, “GmbH”).
    4. Use the most common / widely recognized parent name.

    Output:
    - Return **only** a single JSON object.
    - Keys must be the **exact** original names.
    - Values must be the cleaned parent names.
    - Do **not** include any markdown, comments, or extra fields—just valid JSON.

    Example:
    {{ 
      "Acme Corp., Division A": "Acme Corporation",
      "Globex Inc. R&D": "Globex Corporation"
    }}
    """

    client = load_openai_client()

    try:
        response: Response = client.responses.create(
            model=OPENAI_MODEL,
            input=prompt
        )
        mapping = json.loads(response.output_text)
    except Exception as e:
        print(f"OpenAI error: {e}")
        return patentview_df, fda_df  # Return original if error
    
    if verbose: 
        print(mapping)

    # Step 3: Apply mappings safely
    patentview_df = patentview_df.copy()
    fda_df = fda_df.copy()

    patentview_df['assignee_standardized'] = patentview_df['assignee_organization']
    fda_df['applicant_standardized'] = fda_df['applicant']

    mask_pv = patentview_df['assignee_organization'].isin(mapping)
    mask_fda = fda_df['applicant'].isin(mapping)

    patentview_df.loc[mask_pv, 'assignee_standardized'] = (
        patentview_df.loc[mask_pv, 'assignee_organization'].map(mapping)
    )
    fda_df.loc[mask_fda, 'applicant_standardized'] = (
        fda_df.loc[mask_fda, 'applicant'].map(mapping)
    )

    return patentview_df, fda_df

patentview_df, fda_df = normalize_company_names(patentview_df, fda_df, verbose = True)





fda_df.head()


def find_common_companies(patentview_df: pd.DataFrame, fda_df: pd.DataFrame, verbose: bool = True) -> set[str]:
    """
    Find companies that appear in both the FDA and PatentView datasets based on standardized names.
    """
    # Get unique standardized company names from each dataset
    patent_companies = set(patentview_df['assignee_standardized'].dropna().unique())
    fda_companies = set(fda_df['applicant_standardized'].dropna().unique())

    # Find intersection
    common_companies = patent_companies.intersection(fda_companies)

    # Optionally print the results
    if verbose:
        print(f"Number of companies in both datasets: {len(common_companies)}")
        print("Companies found in both FDA and PatentView:")
        for company in sorted(common_companies):
            print(company)

    return common_companies

common_companies = find_common_companies(patentview_df, fda_df, verbose=True)






import matplotlib.pyplot as plt
import pandas as pd

def plot_patent_trend_common_companies(
    patentview_df: pd.DataFrame,
    common_companies: set[str],
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots number of patents per year for companies found in both FDA and PatentView datasets.

    Args:
        patentview_df (pd.DataFrame): PatentView data with 'year' and 'assignee_standardized' columns.
        common_companies (set[str]): Set of company names present in both datasets.
        show_inline (bool): If True, calls plt.show() (e.g., for Jupyter use).

    Returns:
        fig (plt.Figure): Matplotlib figure object.
    """
    # 1. Filter relevant patents
    filtered_patentview = patentview_df[
        patentview_df['assignee_standardized'].isin(common_companies)
    ]

    # 2. Count patents per year
    yearly_counts = filtered_patentview.groupby('year').size()

    # 3. Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    yearly_counts.plot(kind='bar', color='skyblue', ax=ax)

    ax.set_title('Patent Activity Over Time (Common Companies)', fontsize=16)
    ax.set_xlabel('Year', fontsize=14)
    ax.set_ylabel('Number of Patents', fontsize=14)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
    ax.grid(axis='y', linestyle='--', alpha=0.3)
    plt.tight_layout()

    if show_inline:
        plt.show()

    return fig


# plot_patent_trend_common_companies(patentview_df, common_companies)
fig = plot_patent_trend_common_companies(patentview_df, common_companies, show_inline=True)


import matplotlib.pyplot as plt
import pandas as pd

def plot_fda_trend_common_companies(
    fda_df: pd.DataFrame,
    common_companies: set[str],
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots the number of FDA 510(k) submissions per year for companies 
    found in both FDA and PatentView datasets.

    Args:
        fda_df (pd.DataFrame): FDA data with 'applicant_standardized' and 'year' columns.
        common_companies (set[str]): Set of overlapping company names.
        show_inline (bool): If True, calls plt.show() for Jupyter notebooks.

    Returns:
        fig (plt.Figure): Matplotlib figure object.
    """
    # 1. Filter to common companies
    filtered_fda = fda_df[fda_df['applicant_standardized'].isin(common_companies)]

    # 2. Group and count by year
    yearly_counts = filtered_fda.groupby('year').size()

    # 3. Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    yearly_counts.plot(kind='bar', color='lightgreen', ax=ax)

    ax.set_title('FDA Submission Activity Over Time (Common Companies)', fontsize=16)
    ax.set_xlabel('Year', fontsize=14)
    ax.set_ylabel('Number of Submissions', fontsize=14)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
    ax.grid(axis='y', linestyle='--', alpha=0.3)
    plt.tight_layout()

    if show_inline:
        plt.show()

    return fig

fig = plot_fda_trend_common_companies(fda_df, common_companies, show_inline=True)






import pandas as pd

def get_top_cpc_groups_by_companies(
    patent_df: pd.DataFrame,
    companies: list[str],
    top_n: int = 20
) -> pd.DataFrame:
    """
    Returns the top CPC groups based on patent counts for a given list of companies.
    """
    # Filter for selected companies
    filtered_df = patent_df[patent_df['assignee_standardized'].isin(companies)]

    # Group by CPC code and count
    top_cpc_groups = (
        filtered_df
        .groupby(['cpc_group_id', 'cpc_group_title'])
        .size()
        .reset_index(name='count')
        .sort_values(by='count', ascending=False)
    )

    return top_cpc_groups.head(top_n)

all_fda_companies = list(patentview_df["assignee_standardized"].unique())
print("The number of companies in the patentdata base:",  len(all_fda_companies))

# pd.set_option('display.max_colwidth', None)  # Show full text in column


get_top_cpc_groups_by_companies(patentview_df, all_fda_companies)


import seaborn as sns

# Group by year and CPC group ID, count patents
grouped = patentview_df.groupby(['year', 'cpc_group_id']).size().reset_index(name='count')

pivot_df = grouped.pivot(index='year', columns='cpc_group_id', values='count').fillna(0)

# Optional: limit to top CPCs
top_cpcs = pivot_df.sum().nlargest(15).index
pivot_df = pivot_df[top_cpcs]

plt.figure(figsize=(14, 6))
sns.heatmap(pivot_df, cmap="YlGnBu", linewidths=0.5)
plt.title("Patent Activity Heatmap by CPC Group and Year")
plt.xlabel("CPC Group ID")
plt.ylabel("Year")
plt.tight_layout()
plt.show()





patent_companies = set(patentview_df['assignee_standardized'].dropna().unique())
fda_companies = set(fda_df['applicant_standardized'].dropna().unique())
# Find intersection (companies present in both datasets)
companies_in_fda_and_patent = patent_companies.intersection(fda_companies)

get_top_cpc_groups_by_companies(patentview_df, list(companies_in_fda_and_patent))





import datetime
import pandas as pd

# TODO: view the list of patent by selecting a company

# Get current year
current_year: int  = datetime.datetime.now().year

# Threshold year: last 10 years
threshold_year: int = current_year - 10 # Variable pour streamlit

# Group by assignee and get their first patent year
first_patent_year: pd.DataFrame = patentview_df.groupby('assignee_standardized')['year'].min().reset_index()

# Filter companies whose first filing year is >= threshold year
newcomers_10y: pd.DataFrame = first_patent_year[first_patent_year['year'] >= threshold_year]

# Sort for ranking
newcomers_10y = newcomers_10y.sort_values(by='year').reset_index(drop=True)

# Display
print(f"Number of newcomers in last 10 years: {len(newcomers_10y)}")
newcomers_10y.head(20)


import numpy as np
# Get list of newcomer companies
newcomer_companies: np.ndarray = list(newcomers_10y['assignee_standardized'].unique())

get_top_cpc_groups_by_companies(patentview_df, newcomer_companies)








fda_df.head()


import pandas as pd
import matplotlib.pyplot as plt

def plot_fda_submissions_over_time(
    fda_df: pd.DataFrame,
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots FDA 510(k) submissions per year and returns the Matplotlib figure.
    """
    # Make a copy so we don’t modify the original
    df = fda_df.copy()
    # Ensure 'decision_date' is datetime
    df['decision_date'] = pd.to_datetime(df['decision_date'], errors='coerce')
    df['decision_year'] = df['decision_date'].dt.year

    # Count submissions per year
    yearly_counts = (
        df
        .groupby('decision_year')
        .size()
        .reset_index(name='count')
        .dropna(subset=['decision_year'])
    )

    # Build the figure
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(yearly_counts['decision_year'],
            yearly_counts['count'],
            marker='o',
            linestyle='-')
    ax.set_title('FDA 510(k) Submissions per Year', fontsize=16)
    ax.set_xlabel('Year', fontsize=14)
    ax.set_ylabel('Number of Submissions', fontsize=14)
    ax.grid(True)
    plt.tight_layout()

    # If in a notebook, show immediately
    if show_inline:
        plt.show()

    return fig

fig = plot_fda_submissions_over_time(fda_df, show_inline=True)


# fda_df['applicant_stripped'] = fda_df['applicant_standardized'].astype(str).str.strip()
print(fda_df['applicant_standardized'].value_counts().head(10))






import pandas as pd
import matplotlib.pyplot as plt

def plot_top_fda_applicants(
    fda_df: pd.DataFrame,
    top_n: int = 10,
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots the top N FDA 510(k) applicants (cleaned) as a horizontal bar chart.

    Args:
        fda_df (pd.DataFrame): DataFrame with an 'applicant_standardized' column.
        top_n (int): Number of top applicants to show (default 10).
        show_inline (bool): If True, calls plt.show() so notebooks render the plot immediately.

    Returns:
        fig (plt.Figure): The Matplotlib figure object.
    """
    # Copy to avoid mutating original
    df = fda_df.copy()

    # Count submissions per applicant
    top_applicants = (
        df['applicant_standardized']
        .value_counts()
        .head(top_n)
    )

    # Build the figure
    fig, ax = plt.subplots(figsize=(10, 6))
    top_applicants.plot(kind='barh', ax=ax)
    ax.set_xlabel("Number of 510(k) Submissions", fontsize=14)
    ax.set_ylabel("Applicant", fontsize=14)
    ax.set_title(f"Top {top_n} FDA 510(k) Applicants (Cleaned)", fontsize=16)
    ax.invert_yaxis()  # highest at top
    plt.tight_layout()

    # Show inline in notebooks
    if show_inline:
        plt.show()

    return fig

fig = plot_top_fda_applicants(fda_df, top_n=10, show_inline=True)


fda_df.head()





import pandas as pd
import matplotlib.pyplot as plt

def plot_fda_approvals_by_product_code(
    fda_df: pd.DataFrame,
    show_inline: bool = False
) -> plt.Figure:
    """
    Plots a stacked bar chart of FDA device approvals per year, broken out by product code.

    Args:
        fda_df (pd.DataFrame): DataFrame with 'decision_date' and 'product_code' columns.
        show_inline (bool): If True, calls plt.show() so notebooks render the plot immediately.

    Returns:
        fig (plt.Figure): The Matplotlib figure object.
    """
    # 1. Prepare data
    df = fda_df.copy()
    df['decision_date'] = pd.to_datetime(df['decision_date'], errors='coerce')
    df['decision_year'] = df['decision_date'].dt.year

    # 2. Group by year and product code
    product_year_trend = (
        df
        .groupby(['decision_year', 'product_code'])
        .size()
        .unstack(fill_value=0)
    )

    # 3. Choose a colormap with enough distinct colors
    n_codes = product_year_trend.shape[1]
    cmap = plt.get_cmap('tab20', n_codes)

    # 4. Plot
    fig, ax = plt.subplots(figsize=(12, 6))
    product_year_trend.plot(
        kind='bar',
        stacked=True,
        colormap=cmap,
        ax=ax
    )

    # 5. Decorations
    ax.set_title("FDA Device Approvals by Product Code per Year", fontsize=16)
    ax.set_xlabel("Year", fontsize=14)
    ax.set_ylabel("Number of Approvals", fontsize=14)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)
    ax.grid(axis='y', linestyle='--', alpha=0.3)
    ax.legend(
        title="Product Code",
        bbox_to_anchor=(1.02, 1),
        loc='upper left',
        fontsize='small'
    )
    plt.tight_layout()

    # 6. Show inline for notebooks
    if show_inline:
        plt.show()

    return fig
fig = plot_fda_approvals_by_product_code(fda_df, show_inline=True)


# TODO: add regulation description
# TODO: add select a product code and show companies involved
# 1. Select only the two columns and drop duplicate rows
unique_codes = fda_df[['product_code','product_description']].drop_duplicates()

# 2. Print the DataFrame of unique pairs
print(unique_codes)
